{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:01.883225Z",
     "start_time": "2017-08-01T15:34:00.663331Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the training and testing data from the given CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:01.937140Z",
     "start_time": "2017-08-01T15:34:01.885337Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# X should not contain any of the label columns and y is simply the count\n",
    "train_data = pandas.read_csv('data/train.csv')\n",
    "X = train_data.drop(['count', 'casual', 'registered'], axis=1)\n",
    "y = train_data['count']\n",
    "\n",
    "test_data = pandas.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The scoring function is the Root Mean Squared Logarithmic Error given by\n",
    "\n",
    "$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $\n",
    "\n",
    "Where\n",
    "\n",
    "* $n$ is the number of hours in the test set\n",
    "* $pi$ is your predicted count\n",
    "* $ai$ is the actual count\n",
    "* $log(x)$ is the natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:01.948782Z",
     "start_time": "2017-08-01T15:34:01.939056Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_):\n",
    "    log1 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y]))\n",
    "    log2 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return numpy.sqrt(numpy.mean(calc))\n",
    "\n",
    "# create a custom scorer to be used in grid search, etc\n",
    "scorer = metrics.make_scorer(score_func=rmsle, \n",
    "                             greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "In order to submit to Kaggle we have to generate predictions from the test set and output them to a file with the following format\n",
    "\n",
    "~~~~\n",
    "datetime,count\n",
    "2011-01-20 00:00:00,0\n",
    "2011-01-20 01:00:00,0\n",
    "2011-01-20 02:00:00,0\n",
    "...\n",
    "...\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:01.959814Z",
     "start_time": "2017-08-01T15:34:01.950297Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def generate_kaggle_submission(\n",
    "        transformer, \n",
    "        regressor, \n",
    "        X_train, \n",
    "        y_train,\n",
    "        test_data):\n",
    "    \n",
    "    # train the final model on the transformed data\n",
    "    regressor.fit(transformer(X_train), \n",
    "                  y_train)\n",
    "\n",
    "    # create a dataframe containing the datetimes to predict and then\n",
    "    # add the predictions from the trained pipeline\n",
    "    predictions = pandas.DataFrame(test_data['datetime'])\n",
    "    predictions['count'] = regressor.predict(transformer(test_data)).astype('int')\n",
    "\n",
    "    # create a submission file from the result tagged with the current time\n",
    "    predictions.to_csv('submissions/submission{0}.csv'.format(str(int(time.time()))), \n",
    "                       sep=',', \n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data\n",
    "\n",
    "Split out the given training data into a train and a test set and use all of the available parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:01.977522Z",
     "start_time": "2017-08-01T15:34:01.961495Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8708 training examples and 2178 testing examples\n"
     ]
    }
   ],
   "source": [
    "def partition_train_test(\n",
    "        X, \n",
    "        y,\n",
    "        split_percentage = .8):\n",
    "    mask = numpy.random.rand(len(X)) < split_percentage\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    X_test = X[~mask]\n",
    "    y_test = y[~mask]\n",
    "    \n",
    "    print('{0} training examples and {1} testing examples'.format(len(X_train), \n",
    "                                                                  len(X_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = partition_train_test(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Engineering\n",
    "\n",
    "All of the data is already numeric except for datetime. Replace the datetime with distinct numeric parameters for hour, day, month and year. Then display the summary of the data as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:02.084965Z",
     "start_time": "2017-08-01T15:34:01.979540Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             season       holiday    workingday       weather         temp  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.00000   \n",
      "mean       2.506614      0.028569      0.680875      1.418427     20.23086   \n",
      "std        1.116174      0.166599      0.466159      0.633839      7.79159   \n",
      "min        1.000000      0.000000      0.000000      1.000000      0.82000   \n",
      "25%        2.000000      0.000000      0.000000      1.000000     13.94000   \n",
      "50%        3.000000      0.000000      1.000000      1.000000     20.50000   \n",
      "75%        4.000000      0.000000      1.000000      2.000000     26.24000   \n",
      "max        4.000000      1.000000      1.000000      4.000000     41.00000   \n",
      "\n",
      "              atemp      humidity     windspeed          hour           day  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
      "mean      23.655084     61.886460     12.799395     11.541613      9.992559   \n",
      "std        8.474601     19.245033      8.164537      6.915838      5.476608   \n",
      "min        0.760000      0.000000      0.000000      0.000000      1.000000   \n",
      "25%       16.665000     47.000000      7.001500      6.000000      5.000000   \n",
      "50%       24.240000     62.000000     12.998000     12.000000     10.000000   \n",
      "75%       31.060000     77.000000     16.997900     18.000000     15.000000   \n",
      "max       45.455000    100.000000     56.996900     23.000000     19.000000   \n",
      "\n",
      "              month          year  \n",
      "count  10886.000000  10886.000000  \n",
      "mean       6.521495   2011.501929  \n",
      "std        3.444373      0.500019  \n",
      "min        1.000000   2011.000000  \n",
      "25%        4.000000   2011.000000  \n",
      "50%        7.000000   2012.000000  \n",
      "75%       10.000000   2012.000000  \n",
      "max       12.000000   2012.000000  \n"
     ]
    }
   ],
   "source": [
    "def simple_feature_eng(data):\n",
    "    copy = data.copy()\n",
    "    copy['hour'] = copy.datetime.apply(lambda x: x.split()[1].split(':')[0]).astype('int')\n",
    "    copy['day'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[2]).astype('int')\n",
    "    copy['month'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[1]).astype('int')\n",
    "    copy['year'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[0]).astype('int')\n",
    "    copy = copy.drop(['datetime'], axis=1)\n",
    "    return copy\n",
    "\n",
    "print(simple_feature_eng(X).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Regressor\n",
    "\n",
    "Decision Tree ensembles, particularly Boosted Decision Trees, have fairly good performance over a wide variety of use cases as demonstrated [here](https://ucb-mids.s3.amazonaws.com/prod/DATASCI+W207+Intro+to+Machine+Learning/Readings/caruana.icml06.pdf). Since the values for count have to be both non-negative and an integer we will subclass the Gradient Boosting Regressor to force the predictions to accomidate that requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:34:02.092802Z",
     "start_time": "2017-08-01T15:34:02.087101Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class PositiveIntegerGradientBoostingRegressor(GradientBoostingRegressor):\n",
    "    def predict(\n",
    "            self, \n",
    "            X):\n",
    "        prediction = super(\n",
    "            PositiveIntegerGradientBoostingRegressor, \n",
    "            self).predict(X)\n",
    "        return numpy.around(prediction.clip(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit\n",
    "\n",
    "We will use GridSearch to tune over a range values of max depth for a Gradient Descent Boosted Decision Tree regressor preceeded by our initial feature engineering transformer. We will also use the given RMSLE error function as a custom scoring function. Fit the model and evaluate the resulting predictions on the held out dev data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:58:19.092294Z",
     "start_time": "2017-08-01T15:57:36.893189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 12 features\n",
      "Best Parameters: {'reg__max_depth': 9}\n",
      "RMSLE: 0.35943645500757565\n"
     ]
    }
   ],
   "source": [
    "def simple_grid_search(\n",
    "        regressor, \n",
    "        transformer,\n",
    "        param_grid, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_dev, \n",
    "        y_dev):\n",
    "    pipeline = Pipeline([('reg', regressor)])\n",
    "\n",
    "    model = model_selection.GridSearchCV(pipeline, \n",
    "                                         param_grid, \n",
    "                                         scorer,\n",
    "                                         n_jobs=4)\n",
    "    \n",
    "    print('Transforming dataset with {0} features'.format(len(X_train.columns)))\n",
    "    transformed_X_train = transformer(X_train)\n",
    "    transformed_X_dev = transformer(X_dev)\n",
    "    \n",
    "    print('Fitting model with {0} features'.format(len(transformed_X_train.columns)))\n",
    "    model.fit(transformed_X_train,\n",
    "              y_train)\n",
    "    \n",
    "    print('Best Parameters: {0}'.format(model.best_params_))\n",
    "    print('RMSLE: {0}'.format(rmsle(y_dev, \n",
    "                                    model.predict(transformed_X_dev))))\n",
    "    return model.best_params_\n",
    "\n",
    "hyperparameters = simple_grid_search(\n",
    "    PositiveIntegerGradientBoostingRegressor(n_estimators=100), \n",
    "    simple_feature_eng,\n",
    "    [{'reg__max_depth': list(range(1, 15))}],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Submission Generation\n",
    "Generate the first submission to Kaggle. This resulted in a score of **.50555**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-29T13:06:08.917524Z",
     "start_time": "2017-07-29T13:05:07.217751Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_kaggle_submission(simple_feature_eng,\n",
    "                           PositiveIntegerGradientBoostingRegressor(\n",
    "                               n_estimators=1000, \n",
    "                               max_depth=hyperparameters['reg__max_depth']),\n",
    "                           X,\n",
    "                           y,\n",
    "                           test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "#### 1. Hourly trend of bike demand \n",
    "The figure below shows the trend of bike demand aggregated hourly for the traininig dataset. The trend highlights the gradual increase in demand as we apporach morning office hours, tapering off during the day time. the demand again picks up in after office hours.\n",
    "<img src=\"HourlyBikeCountTrend.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 2. Hourly trend of Registered and Casual bike demand\n",
    "The figure below shows the trend of bike demand aggregated hourly for Casual and Registered bike users over the entire traininig dataset. The figure highlights the differences of trends observed for these two groups. The registered users have similar trend menioned in point 1 above. The demand for casual users peaks in the office hours and trails down during the non office hours. \n",
    "<img src=\"HourlyBikeCountTrendRegisteredVsCasual.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 3. Weekday trend of Registered and Casual bike demand\n",
    "The figure below shows the demand curve for Casual and Registered users for day of a week aggregated for the training data set. The demand for casual users is high on weekends while that of registered users is high during weekdays.\n",
    "<img src=\"WkdayBikeCountTrendRegisteredVsCasual.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "### 4. Impact of working and non-working day on demand from registered and casual bikers\n",
    "The trend below highlghts the differences on the bike demand based on working day and non working day. Casual and registered bike users exhibit similar characteristics on non working day.\n",
    "<img src=\"WorkNonworkBikeCountTrendRegisteredVsCasual.png\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Based on the above exploratory analysis and visual inspection of the data we have made the following observations:\n",
    "\n",
    "1. The behavior of the registered bikers is very different from the behavior of the casual bikers in terms of time of day, date of week, etc.\n",
    "2. The training set of data consists of only the first 20 days of any given month and the testing data always consists of the last 7-11 days.\n",
    "3. The mean values and standard deviations vary widely among different parameters.\n",
    "4. Generally speaking model displays some signs of overfitting given the training RMSE of .35 and the testing RMSE of over .5. \n",
    "\n",
    "## Plan\n",
    "1. To account for the difference in behavior between the two groups we will construct 2 completely distinct models and sum the resulting predictions to get the overall demand. In this way we can account for the disparate behavior between the two customer types.\n",
    "2. We will create an ensemble of sorts by using a purely time-series method to forecast the demand on the last 7-11 days of each month. Because the monthly data itself is stationary and fairly periodic this should give us a reasonable (on the average) estimate of the remaining days demand each month. This data will then become part of training data and hopefully will alleviate some of the inaccuracies caused by prediction from ranges of parameters not in the training set.\n",
    "3. We will simply standardize all of the features to zero mean and unit variance.\n",
    "4. We may try choosing some other core model than Boosted Decision Trees even though this variant of Decision Trees (limiting depth, iterating over multiple trees) generally limits overfitting. A simple model that is less inclined to overfit such as Logistic Regression may work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Revised Feature Engineering\n",
    "To make an incremental change towards the above plan we will do the following:\n",
    "\n",
    "* Since there are only 2 years worth of data we can reduce the year and month data down to a single parameter month_count\n",
    "* Simplify the 1-20: 21-31 day split and still maintain the periodic nature of the data is to drop the day of the month variable and create a day of the week variable. This is a predecessor to the time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-01T15:59:11.964763Z",
     "start_time": "2017-08-01T15:58:31.938937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 11 features\n",
      "Best Parameters: {'reg__max_depth': 9}\n",
      "RMSLE: 0.33334768623979777\n"
     ]
    }
   ],
   "source": [
    "def get_date(my_datetime):\n",
    "    return datetime.strptime(\n",
    "        my_datetime, \n",
    "        '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def revised_feature_eng(data):\n",
    "    copy = data.copy()\n",
    "    copy['hour'] = copy.datetime.apply(lambda x: get_date(x).hour).astype('int')    \n",
    "    copy['month_count'] = copy.datetime.apply(lambda x: (get_date(x).year-2011)*12 + get_date(x).month).astype('int')\n",
    "    copy['day_of_week'] = copy.datetime.apply(lambda x: get_date(x).weekday())\n",
    "    copy = copy.drop(['datetime'], axis=1)\n",
    "    return copy\n",
    "\n",
    "hyperparameters = simple_grid_search(\n",
    "    PositiveIntegerGradientBoostingRegressor(n_estimators=100), \n",
    "    revised_feature_eng,\n",
    "    [{'reg__max_depth': list(range(1, 15))}],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 2\n",
    "Interestingly in this submission the RMSLE score went up to **.55832**. It appears as if the day_of_the_week parameter is causing problems with the testing data despite the score improving on the held out development data. This is a more solid indicator of overfitting and that changing the underlying model may be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:53:00.941905Z",
     "start_time": "2017-07-22T17:51:42.642649Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_kaggle_submission(revised_feature_eng,\n",
    "                           PositiveIntegerGradientBoostingRegressor(\n",
    "                               n_estimators=1000, \n",
    "                               max_depth=hyperparameters['reg__max_depth']),\n",
    "                           X,\n",
    "                           y,\n",
    "                           test_data)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
