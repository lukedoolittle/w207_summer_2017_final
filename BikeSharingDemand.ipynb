{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:38.295411Z",
     "start_time": "2017-07-22T17:43:38.289847Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the training and testing data from the given CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:38.885391Z",
     "start_time": "2017-07-22T17:43:38.847640Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# X should not contain any of the label columns and y is simply the count\n",
    "train_data = pandas.read_csv('data/train.csv')\n",
    "X = train_data.drop(['count', 'casual', 'registered'], axis=1)\n",
    "y = train_data['count']\n",
    "\n",
    "test_data = pandas.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The scoring function is the Root Mean Squared Logarithmic Error given by\n",
    "\n",
    "$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $\n",
    "\n",
    "Where\n",
    "\n",
    "* $n$ is the number of hours in the test set\n",
    "* $pi$ is your predicted count\n",
    "* $ai$ is the actual count\n",
    "* $log(x)$ is the natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:39.836557Z",
     "start_time": "2017-07-22T17:43:39.826632Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_):\n",
    "    log1 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y]))\n",
    "    log2 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return numpy.sqrt(numpy.mean(calc))\n",
    "\n",
    "# create a custom scorer to be used in grid search, etc\n",
    "scorer = metrics.make_scorer(score_func=rmsle, \n",
    "                             greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "In order to submit to Kaggle we have to generate predictions from the test set and output them to a file with the following format\n",
    "\n",
    "~~~~\n",
    "datetime,count\n",
    "2011-01-20 00:00:00,0\n",
    "2011-01-20 01:00:00,0\n",
    "2011-01-20 02:00:00,0\n",
    "...\n",
    "...\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:40.781285Z",
     "start_time": "2017-07-22T17:43:40.772133Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def generate_kaggle_submission(\n",
    "        transformer, \n",
    "        regressor, \n",
    "        X_train, \n",
    "        y_train,\n",
    "        test_data):\n",
    "    \n",
    "    # train the final model on the transformed data\n",
    "    regressor.fit(transformer(X_train), \n",
    "                  y_train)\n",
    "\n",
    "    # create a dataframe containing the datetimes to predict and then\n",
    "    # add the predictions from the trained pipeline\n",
    "    predictions = pandas.DataFrame(test_data['datetime'])\n",
    "    predictions['count'] = regressor.predict(transformer(test_data)).astype('int')\n",
    "\n",
    "    # create a submission file from the result tagged with the current time\n",
    "    predictions.to_csv('submissions/submission{0}.csv'.format(str(int(time.time()))), \n",
    "                       sep=',', \n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data\n",
    "\n",
    "Split out the given training data into a train and a test set and use all of the available parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:41.913183Z",
     "start_time": "2017-07-22T17:43:41.898696Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8729 training examples and 2157 testing examples\n"
     ]
    }
   ],
   "source": [
    "def partition_train_test(\n",
    "        X, \n",
    "        y,\n",
    "        split_percentage = .8):\n",
    "    mask = numpy.random.rand(len(X)) < split_percentage\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    X_test = X[~mask]\n",
    "    y_test = y[~mask]\n",
    "    \n",
    "    print('{0} training examples and {1} testing examples'.format(len(X_train), \n",
    "                                                                  len(X_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = partition_train_test(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Engineering\n",
    "\n",
    "All of the data is already numeric except for datetime. Replace the datetime with distinct numeric parameters for hour, day, month and year. Then display the summary of the data as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:43.687078Z",
     "start_time": "2017-07-22T17:43:43.585073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             season       holiday    workingday       weather         temp  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.00000   \n",
      "mean       2.506614      0.028569      0.680875      1.418427     20.23086   \n",
      "std        1.116174      0.166599      0.466159      0.633839      7.79159   \n",
      "min        1.000000      0.000000      0.000000      1.000000      0.82000   \n",
      "25%        2.000000      0.000000      0.000000      1.000000     13.94000   \n",
      "50%        3.000000      0.000000      1.000000      1.000000     20.50000   \n",
      "75%        4.000000      0.000000      1.000000      2.000000     26.24000   \n",
      "max        4.000000      1.000000      1.000000      4.000000     41.00000   \n",
      "\n",
      "              atemp      humidity     windspeed          hour           day  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
      "mean      23.655084     61.886460     12.799395     11.541613      9.992559   \n",
      "std        8.474601     19.245033      8.164537      6.915838      5.476608   \n",
      "min        0.760000      0.000000      0.000000      0.000000      1.000000   \n",
      "25%       16.665000     47.000000      7.001500      6.000000      5.000000   \n",
      "50%       24.240000     62.000000     12.998000     12.000000     10.000000   \n",
      "75%       31.060000     77.000000     16.997900     18.000000     15.000000   \n",
      "max       45.455000    100.000000     56.996900     23.000000     19.000000   \n",
      "\n",
      "              month          year  \n",
      "count  10886.000000  10886.000000  \n",
      "mean       6.521495   2011.501929  \n",
      "std        3.444373      0.500019  \n",
      "min        1.000000   2011.000000  \n",
      "25%        4.000000   2011.000000  \n",
      "50%        7.000000   2012.000000  \n",
      "75%       10.000000   2012.000000  \n",
      "max       12.000000   2012.000000  \n"
     ]
    }
   ],
   "source": [
    "def simple_feature_eng(data):\n",
    "    copy = data.copy()\n",
    "    copy['hour'] = copy.datetime.apply(lambda x: x.split()[1].split(':')[0]).astype('int')\n",
    "    copy['day'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[2]).astype('int')\n",
    "    copy['month'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[1]).astype('int')\n",
    "    copy['year'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[0]).astype('int')\n",
    "    copy = copy.drop(['datetime'], axis=1)\n",
    "    return copy\n",
    "\n",
    "print(simple_feature_eng(X).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Regressor\n",
    "\n",
    "Decision Tree ensembles, particularly Boosted Decision Trees, have fairly good performance over a wide variety of use cases as demonstrated [here](https://ucb-mids.s3.amazonaws.com/prod/DATASCI+W207+Intro+to+Machine+Learning/Readings/caruana.icml06.pdf). Since the values for count have to be both non-negative and an integer we will subclass the Gradient Boosting Regressor to force the predictions to accomidate that requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:43:45.204493Z",
     "start_time": "2017-07-22T17:43:45.199546Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositiveIntegerGradientBoostingRegressor(GradientBoostingRegressor):\n",
    "    def predict(\n",
    "            self, \n",
    "            X):\n",
    "        prediction = super(\n",
    "            PositiveIntegerGradientBoostingRegressor, \n",
    "            self).predict(X)\n",
    "        return numpy.around(prediction.clip(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit\n",
    "\n",
    "We will use GridSearch to tune over a range values of max depth for a Gradient Descent Boosted Decision Tree regressor preceeded by our initial feature engineering transformer. We will also use the given RMSLE error function as a custom scoring function. Fit the model and evaluate the resulting predictions on the held out dev data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:56:36.774319Z",
     "start_time": "2017-07-22T17:55:54.016767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 12 features\n",
      "Best Parameters: {'reg__max_depth': 10}\n",
      "RMSLE: 0.33345986936344557\n"
     ]
    }
   ],
   "source": [
    "def simple_grid_search(\n",
    "        regressor, \n",
    "        transformer,\n",
    "        param_grid, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_dev, \n",
    "        y_dev):\n",
    "    pipeline = Pipeline([('reg', regressor)])\n",
    "\n",
    "    model = model_selection.GridSearchCV(pipeline, \n",
    "                                         param_grid, \n",
    "                                         scorer,\n",
    "                                         n_jobs=4)\n",
    "    \n",
    "    print('Transforming dataset with {0} features'.format(len(X_train.columns)))\n",
    "    transformed_X_train = transformer(X_train)\n",
    "    transformed_X_dev = transformer(X_dev)\n",
    "    \n",
    "    print('Fitting model with {0} features'.format(len(transformed_X_train.columns)))\n",
    "    model.fit(transformed_X_train,\n",
    "              y_train)\n",
    "    \n",
    "    print('Best Parameters: {0}'.format(model.best_params_))\n",
    "    print('RMSLE: {0}'.format(rmsle(y_dev, \n",
    "                                    model.predict(transformed_X_dev))))\n",
    "    return model.best_params_\n",
    "\n",
    "hyperparameters = simple_grid_search(\n",
    "    PositiveIntegerGradientBoostingRegressor(n_estimators=100), \n",
    "    simple_feature_eng,\n",
    "    [{'reg__max_depth': list(range(1, 15))}],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Submission Generation\n",
    "Generate the first submission to Kaggle. This resulted in a score of **.50555**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:44:10.159335Z",
     "start_time": "2017-07-22T17:44:10.154349Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-145-3f95efc84d7e>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-145-3f95efc84d7e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    test_data\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "generate_kaggle_submission(simple_feature_eng,\n",
    "                           PositiveIntegerGradientBoostingRegressor(\n",
    "                               n_estimators=1000, \n",
    "                               max_depth=hyperparameters['reg__max_depth']),\n",
    "                           X,\n",
    "                           y,\n",
    "                           test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Revised Feature Engineering\n",
    "After looking at the data there are a couple of changes to the feature engineering we can make:\n",
    "\n",
    "* Since there are only 2 years worth of data we can reduce the year and month data down to a single parameter month_count\n",
    "* The training data is only from the first 20 days of each month, and the test data is always from the last 7-11 days. This may be problematic because the model is seeing values outside of its trained range for day of month on EVERY testing example. One way to simplify this and still maintain the periodic nature of the data is to drop the day of the month variable and create a day of the week variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:51:37.749737Z",
     "start_time": "2017-07-22T17:50:54.640233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 11 features\n",
      "Best Parameters: {'reg__max_depth': 11}\n",
      "RMSLE: 0.3300200190195893\n"
     ]
    }
   ],
   "source": [
    "def get_date(my_datetime):\n",
    "    return datetime.strptime(\n",
    "        my_datetime, \n",
    "        '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def revised_feature_eng(data):\n",
    "    copy = data.copy()\n",
    "    copy['hour'] = copy.datetime.apply(lambda x: get_date(x).hour).astype('int')    \n",
    "    copy['month_count'] = copy.datetime.apply(lambda x: (get_date(x).year-2011)*12 + get_date(x).month).astype('int')\n",
    "    copy['day_of_week'] = copy.datetime.apply(lambda x: get_date(x).weekday())\n",
    "    copy = copy.drop(['datetime'], axis=1)\n",
    "    return copy\n",
    "\n",
    "hyperparameters = simple_grid_search(\n",
    "    PositiveIntegerGradientBoostingRegressor(n_estimators=100), \n",
    "    revised_feature_eng,\n",
    "    [{'reg__max_depth': list(range(1, 15))}],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission 2\n",
    "Interestingly in this submission the RMSLE score went up to **.55832**. It appears as if the day_of_the_week parameter is causing problems with the testing data despite the score improving on the held out development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T17:53:00.941905Z",
     "start_time": "2017-07-22T17:51:42.642649Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_kaggle_submission(revised_feature_eng,\n",
    "                           PositiveIntegerGradientBoostingRegressor(\n",
    "                               n_estimators=1000, \n",
    "                               max_depth=hyperparameters['reg__max_depth']),\n",
    "                           X,\n",
    "                           y,\n",
    "                           test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
