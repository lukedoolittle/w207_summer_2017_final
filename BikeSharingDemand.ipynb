{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T14:29:46.378463Z",
     "start_time": "2017-08-19T14:29:46.369056Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the training and testing data from the given CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.173831Z",
     "start_time": "2017-08-19T12:33:41.123175Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# X should not contain any of the label columns and y is simply the count\n",
    "train_data = pandas.read_csv('data/train.csv')\n",
    "X = train_data.drop(['count', 'casual', 'registered'], axis=1)\n",
    "y = train_data['count']\n",
    "y1 = train_data['casual']\n",
    "y2 = train_data['registered']\n",
    "\n",
    "test_data = pandas.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The scoring function is the Root Mean Squared Logarithmic Error given by\n",
    "\n",
    "$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $\n",
    "\n",
    "Where\n",
    "\n",
    "* $n$ is the number of hours in the test set\n",
    "* $pi$ is your predicted count\n",
    "* $ai$ is the actual count\n",
    "* $log(x)$ is the natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.188442Z",
     "start_time": "2017-08-19T12:33:41.176721Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_):\n",
    "    log1 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y]))\n",
    "    log2 = numpy.nan_to_num(numpy.array([numpy.log(v + 1) \n",
    "                                         for v \n",
    "                                         in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return numpy.sqrt(numpy.mean(calc))\n",
    "\n",
    "# create a custom scorer to be used in grid search, etc\n",
    "scorer = metrics.make_scorer(score_func=rmsle, \n",
    "                             greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "In order to submit to Kaggle we have to generate predictions from the test set and output them to a file with the following format\n",
    "\n",
    "~~~~\n",
    "datetime,count\n",
    "2011-01-20 00:00:00,0\n",
    "2011-01-20 01:00:00,0\n",
    "2011-01-20 02:00:00,0\n",
    "...\n",
    "...\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.200747Z",
     "start_time": "2017-08-19T12:33:41.190253Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def generate_kaggle_submission(\n",
    "        transformer, \n",
    "        regressor, \n",
    "        X_train, \n",
    "        y_train,\n",
    "        test_data):\n",
    "    \n",
    "    # train the final model on the transformed data\n",
    "    regressor.fit(transformer(X_train), \n",
    "                  y_train)\n",
    "\n",
    "    # create a dataframe containing the datetimes to predict and then\n",
    "    # add the predictions from the trained pipeline\n",
    "    predictions = pandas.DataFrame(test_data['datetime'])\n",
    "    predictions['count'] = regressor.predict(transformer(test_data)).astype('int')\n",
    "\n",
    "    # create a submission file from the result tagged with the current time\n",
    "    predictions.to_csv('submissions/submission{0}.csv'.format(str(int(time.time()))), \n",
    "                       sep=',', \n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Data\n",
    "\n",
    "Split out the given training data into a train and a test set and use all of the available parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.223936Z",
     "start_time": "2017-08-19T12:33:41.202590Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8747 training examples and 2139 testing examples\n"
     ]
    }
   ],
   "source": [
    "def partition_train_test(\n",
    "        X, \n",
    "        y,\n",
    "        y1,\n",
    "        y2,\n",
    "        split_percentage = .8):\n",
    "    mask = numpy.random.rand(len(X)) < split_percentage\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    y1_train = y1[mask]\n",
    "    y2_train = y2[mask]\n",
    "    X_test = X[~mask]\n",
    "    y_test = y[~mask]\n",
    "    y1_test = y1[~mask]\n",
    "    y2_test = y2[~mask]\n",
    "    \n",
    "    print('{0} training examples and {1} testing examples'.format(len(X_train), \n",
    "                                                                  len(X_test)))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y1_train, y1_test, y2_train, y2_test\n",
    "\n",
    "X_train, X_dev, y_train, y_dev, y1_train, y1_dev, y2_train, y2_dev = partition_train_test(X, y, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Engineering\n",
    "\n",
    "All of the data is already numeric except for datetime. Replace the datetime with distinct numeric parameters for hour, day, month and year. Then display the summary of the data as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.325592Z",
     "start_time": "2017-08-19T12:33:41.225550Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             season       holiday    workingday       weather         temp  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.00000   \n",
      "mean       2.506614      0.028569      0.680875      1.418427     20.23086   \n",
      "std        1.116174      0.166599      0.466159      0.633839      7.79159   \n",
      "min        1.000000      0.000000      0.000000      1.000000      0.82000   \n",
      "25%        2.000000      0.000000      0.000000      1.000000     13.94000   \n",
      "50%        3.000000      0.000000      1.000000      1.000000     20.50000   \n",
      "75%        4.000000      0.000000      1.000000      2.000000     26.24000   \n",
      "max        4.000000      1.000000      1.000000      4.000000     41.00000   \n",
      "\n",
      "              atemp      humidity     windspeed          hour           day  \\\n",
      "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
      "mean      23.655084     61.886460     12.799395     11.541613      9.992559   \n",
      "std        8.474601     19.245033      8.164537      6.915838      5.476608   \n",
      "min        0.760000      0.000000      0.000000      0.000000      1.000000   \n",
      "25%       16.665000     47.000000      7.001500      6.000000      5.000000   \n",
      "50%       24.240000     62.000000     12.998000     12.000000     10.000000   \n",
      "75%       31.060000     77.000000     16.997900     18.000000     15.000000   \n",
      "max       45.455000    100.000000     56.996900     23.000000     19.000000   \n",
      "\n",
      "              month          year  \n",
      "count  10886.000000  10886.000000  \n",
      "mean       6.521495   2011.501929  \n",
      "std        3.444373      0.500019  \n",
      "min        1.000000   2011.000000  \n",
      "25%        4.000000   2011.000000  \n",
      "50%        7.000000   2012.000000  \n",
      "75%       10.000000   2012.000000  \n",
      "max       12.000000   2012.000000  \n"
     ]
    }
   ],
   "source": [
    "def get_date(my_datetime):\n",
    "    return datetime.strptime(\n",
    "        my_datetime, \n",
    "        '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def simple_feature_eng(data):\n",
    "    copy = data.copy()\n",
    "    copy['hour'] = copy.datetime.apply(lambda x: x.split()[1].split(':')[0]).astype('int')\n",
    "    copy['day'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[2]).astype('int')\n",
    "    copy['month'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[1]).astype('int')\n",
    "    copy['year'] = copy.datetime.apply(lambda x: x.split()[0].split('-')[0]).astype('int')\n",
    "    copy = copy.drop(['datetime'], axis=1)\n",
    "    return copy\n",
    "\n",
    "print(simple_feature_eng(X).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Regressor\n",
    "\n",
    "Decision Tree ensembles, particularly Boosted Decision Trees, have fairly good performance over a wide variety of use cases as demonstrated [here](https://ucb-mids.s3.amazonaws.com/prod/DATASCI+W207+Intro+to+Machine+Learning/Readings/caruana.icml06.pdf). Since the values for count have to be both non-negative and an integer we will subclass the Gradient Boosting Regressor to force the predictions to accomidate that requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:33:41.332049Z",
     "start_time": "2017-08-19T12:33:41.327589Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class PositiveIntegerGradientBoostingRegressor(GradientBoostingRegressor):\n",
    "    def predict(\n",
    "            self, \n",
    "            X):\n",
    "        prediction = super(\n",
    "            PositiveIntegerGradientBoostingRegressor, \n",
    "            self).predict(X)\n",
    "        return numpy.around(prediction.clip(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit\n",
    "\n",
    "We will use GridSearch to tune over a range values of max depth for a Gradient Descent Boosted Decision Tree regressor preceeded by our initial feature engineering transformer. We will also use the given RMSLE error function as a custom scoring function. Fit the model and evaluate the resulting predictions on the held out dev data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T15:36:58.382026Z",
     "start_time": "2017-08-19T15:36:58.369214Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def simple_grid_search(\n",
    "        regressor, \n",
    "        transformer,\n",
    "        param_grid, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_dev, \n",
    "        y_dev):\n",
    "    pipeline = Pipeline([('reg', regressor)])\n",
    "\n",
    "    model = model_selection.GridSearchCV(pipeline, \n",
    "                                         param_grid, \n",
    "                                         scorer,\n",
    "                                         n_jobs=-1)\n",
    "    \n",
    "    transformed_X_train = transformer(X_train)\n",
    "    transformed_X_dev = transformer(X_dev)\n",
    "    model.fit(transformed_X_train,\n",
    "              y_train)\n",
    "    \n",
    "    print('Best Parameters: {0}'.format(model.best_params_))\n",
    "    print('RMSLE: {0}'.format(rmsle(y_dev, \n",
    "                                    model.predict(transformed_X_dev))))\n",
    "    return model.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T15:08:52.728218Z",
     "start_time": "2017-08-19T15:08:30.207686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 12 features\n",
      "Best Parameters: {'reg__max_depth': 11}\n",
      "RMSLE: 0.3573104315806729\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = simple_grid_search(\n",
    "    PositiveIntegerGradientBoostingRegressor(n_estimators=100), \n",
    "    simple_feature_eng,\n",
    "    [{'reg__max_depth': list(range(3, 12))}],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Submission Generation\n",
    "Generate the first submission to Kaggle. This resulted in a score of **.50555**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-29T13:06:08.917524Z",
     "start_time": "2017-07-29T13:05:07.217751Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best hyperparameters found from simple_grid_search above\n",
    "max_depth = 11\n",
    "\n",
    "generate_kaggle_submission(simple_feature_eng,\n",
    "                           PositiveIntegerGradientBoostingRegressor(\n",
    "                               n_estimators=1000, \n",
    "                               max_depth=max_depth),\n",
    "                           X,\n",
    "                           y,\n",
    "                           test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "#### 1. Hourly trend of bike demand \n",
    "The figure below shows the trend of bike demand aggregated hourly for the traininig dataset. The trend highlights the gradual increase in demand as we apporach morning office hours, tapering off during the day time. the demand again picks up in after office hours.\n",
    "<img src=\"HourlyBikeCountTrend.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 2. Hourly trend of Registered and Casual bike demand\n",
    "The figure below shows the trend of bike demand aggregated hourly for Casual and Registered bike users over the entire traininig dataset. The figure highlights the differences of trends observed for these two groups. The registered users have similar trend menioned in point 1 above. The demand for casual users peaks in the office hours and trails down during the non office hours. \n",
    "<img src=\"HourlyBikeCountTrendRegisteredVsCasual.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 3. Weekday trend of Registered and Casual bike demand\n",
    "The figure below shows the demand curve for Casual and Registered users for day of a week aggregated for the training data set. The demand for casual users is high on weekends while that of registered users is high during weekdays.\n",
    "<img src=\"WkdayBikeCountTrendRegisteredVsCasual.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "### 4. Impact of working and non-working day on demand from registered and casual bikers\n",
    "The trend below highlghts the differences on the bike demand based on working day and non working day. Casual and registered bike users exhibit similar characteristics on non working day.\n",
    "<img src=\"WorkNonworkBikeCountTrendRegisteredVsCasual.png\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Based on the above exploratory analysis and visual inspection of the data we have made the following observations:\n",
    "\n",
    "1. The behavior of the registered bikers is very different from the behavior of the casual bikers in terms of time of day, date of week, etc.\n",
    "2. The training set of data consists of only the first 20 days of any given month and the testing data always consists of the last 7-11 days.\n",
    "3. The mean values and standard deviations vary widely among different parameters.\n",
    "4. Generally speaking model displays some signs of overfitting given the training RMSE of .35 and the testing RMSE of over .5. \n",
    "\n",
    "## Plan\n",
    "1. To account for the difference in behavior between the two groups we will construct 2 completely distinct models and sum the resulting predictions to get the overall demand. In this way we can account for the disparate behavior between the two customer types.\n",
    "2. We will create an ensemble of sorts by using a purely time-series method to forecast the demand on the last 7-11 days of each month. Because the monthly data itself is stationary and fairly periodic this should give us a reasonable (on the average) estimate of the remaining days demand each month. This data will then become part of training data and hopefully will alleviate some of the inaccuracies caused by prediction from ranges of parameters not in the training set.\n",
    "3. We will simply standardize all of the features to zero mean and unit variance.\n",
    "4. We may try choosing some other core model than Boosted Decision Trees even though this variant of Decision Trees (limiting depth, iterating over multiple trees) generally limits overfitting. A simple model that is less inclined to overfit such as Logistic Regression may work better.\n",
    "\n",
    "As an overall note dependant variables tend to have outliers. So we will consider removing outliers and/or predicting a transformed version of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Casual/Registered Model Split\n",
    "Working from part one of the plan we will simply break the prediction down into two models: one predicting the number of registered users in a day and the other predicting the number of casual users. In addition we will take a simple log(x+1) transform on the regressor to normalize for outliers, and reverse the transform for the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T15:17:06.463200Z",
     "start_time": "2017-08-19T15:16:36.322777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset with 9 features\n",
      "Fitting model with 12 features\n",
      "Best Parameters: {'reg__max_depth': 6}\n",
      "RMSLE: 0.13698979192894803\n",
      "Transforming dataset with 9 features\n",
      "Fitting model with 12 features\n",
      "Best Parameters: {'reg__max_depth': 4}\n",
      "RMSLE: 0.07704778792073358\n"
     ]
    }
   ],
   "source": [
    "casual_hyperparameters = simple_grid_search(\n",
    "    GradientBoostingRegressor(n_estimators=100), \n",
    "    simple_feature_eng,\n",
    "    [{'reg__max_depth': list(range(3, 12))}],\n",
    "    X_train,\n",
    "    numpy.log1p(y1_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y1_dev + 1))\n",
    "\n",
    "registered_hyperparameters = simple_grid_search(\n",
    "    GradientBoostingRegressor(n_estimators=100), \n",
    "    simple_feature_eng,\n",
    "    [{'reg__max_depth': list(range(3, 12))}],\n",
    "    X_train,\n",
    "    numpy.log1p(y2_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y2_dev + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Second Submission Generation\n",
    "Generate our second submission to Kaggle for a score. This requires and input-output transformation of the count variable and summing the resulting generation of scores from the casual and registered users. This resulted in a submission score of **.46154** which was a vast improvement over our previous score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T15:20:40.245752Z",
     "start_time": "2017-08-19T15:20:20.834325Z"
    }
   },
   "outputs": [],
   "source": [
    "# best hyperparameters found from simple_grid_search above\n",
    "casual_max_depth = 6\n",
    "registered_max_depth = 4\n",
    "\n",
    "casual_regressor = GradientBoostingRegressor(n_estimators=1000, \n",
    "                                             max_depth=casual_max_depth)\n",
    "registered_regressor = GradientBoostingRegressor(n_estimators=1000, \n",
    "                                                 max_depth=registered_max_depth)\n",
    "\n",
    "# train the final models on the transformed data and the transformed labels\n",
    "casual_regressor.fit(simple_feature_eng(X), \n",
    "                     numpy.log1p(y1 + 1))\n",
    "registered_regressor.fit(simple_feature_eng(X), \n",
    "                         numpy.log1p(y2 + 1))\n",
    "\n",
    "# create a dataframe containing the datetimes to predict\n",
    "predictions = pandas.DataFrame(test_data['datetime'])\n",
    "\n",
    "# predict for both the casual and registered users\n",
    "casual_predictions = casual_regressor.predict(simple_feature_eng(test_data))\n",
    "registered_predictions = registered_regressor.predict(simple_feature_eng(test_data))\n",
    "\n",
    "# reverse the transform for both predictions and then add the result\n",
    "predictions['count'] = (numpy.exp(casual_predictions) - 1 + \n",
    "                        numpy.exp(registered_predictions) - 1).astype('int')\n",
    "\n",
    "# create a submission file from the result tagged with the current time\n",
    "predictions.to_csv('submissions/submission{0}.csv'.format(str(int(time.time()))), \n",
    "                   sep=',', \n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Selection\n",
    "In addition to splitting out the models as done above, we would like to perform a search over different types of models as well as their hyperparameters. The above improvement was primarily due to the log transform of the output data which indicates that the model is somewhat sensitive to outliers. In addition some simple feature engineering changes (not shown in this notebook) improved the score on the training set but worsened the score on the test set. These two phenomenon are classic indicators of model overfitting. In order to combat that overfitting we would like to try a variety of different models. In addition there is some [evidence](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) that simple cross-validation for both model selection and performance estimation can result in further overfitting. Thus we will use nested cross-validation to select a model for the registered and casual users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T14:41:09.577479Z",
     "start_time": "2017-08-19T14:37:35.526247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (casual)                 RMSE           \n",
      "-------------------------------------\n",
      "SVR                            0.3398         \n",
      "RandomForestRegressor          0.1639         \n",
      "GradientBoostingRegressor      0.1571         \n",
      "Ridge                          0.2590         \n",
      "Lasso                          0.2802         \n",
      "Model (registered)             RMSE           \n",
      "-------------------------------------\n",
      "SVR                            0.2712         \n",
      "RandomForestRegressor          0.1173         \n",
      "GradientBoostingRegressor      0.1034         \n",
      "Ridge                          0.2223         \n",
      "Lasso                          0.2415         \n"
     ]
    }
   ],
   "source": [
    "def nested_cross_validation(\n",
    "        models, \n",
    "        params, \n",
    "        transformer,\n",
    "        X, \n",
    "        y):\n",
    "    transformed_X = transformer(X)\n",
    "    scores = [[] for _ in range(len(models))]\n",
    "    \n",
    "    for tr, ts in KFold(len(transformed_X)):\n",
    "        for i, (model, param) in enumerate(zip(models, params)):\n",
    "            model = model_selection.GridSearchCV(\n",
    "                model, \n",
    "                param, \n",
    "                scorer,\n",
    "                n_jobs=4)\n",
    "            model.fit(transformed_X.iloc[tr], \n",
    "                      y[tr])\n",
    "            scores[i].append(rmsle(y[ts], \n",
    "                                   model.predict(transformed_X.iloc[ts])))\n",
    "    return zip(models, np.mean(scores, 1))\n",
    "    \n",
    "\n",
    "regressors = [SVR(), \n",
    "              RandomForestRegressor(),\n",
    "              GradientBoostingRegressor(),\n",
    "              Ridge(),\n",
    "              Lasso()]\n",
    "parameters = [{'C': [0.01, 0.05, 0.1, .5, 1]}, \n",
    "              {'max_depth': list(range(3, 12))},\n",
    "              {'max_depth': list(range(3, 12))},\n",
    "              {'alpha': [0.01, 0.05, 0.1, .5, 1]},\n",
    "              {'alpha': [100, 20, 10, 2, 1]}]\n",
    "\n",
    "casual_results = nested_cross_validation(\n",
    "                    regressors,\n",
    "                    parameters,\n",
    "                    simple_feature_eng,\n",
    "                    X,\n",
    "                    numpy.log1p(y1 + 1))\n",
    "\n",
    "print('{:<30} {:<15}'.format('Model (casual)', 'RMSLE'))\n",
    "print('-------------------------------------')\n",
    "for i in casual_results:\n",
    "    print('{:<30} {:<15,.4f}'.format(i[0].__class__.__name__, \n",
    "                                               i[1]))\n",
    "    \n",
    "registered_results = nested_cross_validation(\n",
    "                        regressors,\n",
    "                        parameters,\n",
    "                        simple_feature_eng,\n",
    "                        X,\n",
    "                        numpy.log1p(y2 + 1))\n",
    "\n",
    "print('\\n')\n",
    "print('{:<30} {:<15}'.format('Model (registered)', 'RMSLE'))\n",
    "print('-------------------------------------')\n",
    "for i in registered_results:\n",
    "    print('{:<30} {:<15,.4f}'.format(i[0].__class__.__name__, \n",
    "                                               i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both casual and registered users Random Forrest and Gradient Descent Boosted Decision Trees produce superior results as compared to any other regression model. Since they are both very similar in terms of their RMSLE we will create a simple ensemble model that averages the output of the two models.\n",
    "\n",
    "To be more thorough with this iteration we perform a grid search over all relevant parameters rather than arbitrarily setting n_estimators in the GradientBoostingRegressor as in previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T15:44:48.420326Z",
     "start_time": "2017-08-19T15:38:30.167438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casual user GradientBoostingRegressor\n",
      "Best Parameters: {'reg__max_depth': 6, 'reg__n_estimators': 100}\n",
      "RMSLE: 0.13703367972045005\n",
      "\n",
      "\n",
      "Registered user GradientBoostingRegressor\n",
      "Best Parameters: {'reg__max_depth': 3, 'reg__n_estimators': 300}\n",
      "RMSLE: 0.0747842138473513\n"
     ]
    }
   ],
   "source": [
    "print('Casual user GradientBoostingRegressor')\n",
    "casual_gb_hyperparameters = simple_grid_search(\n",
    "    GradientBoostingRegressor(), \n",
    "    simple_feature_eng,\n",
    "    {'reg__max_depth': list(range(3, 12)),\n",
    "     'reg__n_estimators': [100, 200, 300, 400, 500, 1000]},\n",
    "    X_train,\n",
    "    numpy.log1p(y1_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y1_dev + 1))\n",
    "\n",
    "print('\\n')\n",
    "print('Registered user GradientBoostingRegressor')\n",
    "registered_gb_hyperparameters = simple_grid_search(\n",
    "    GradientBoostingRegressor(), \n",
    "    simple_feature_eng,\n",
    "    {'reg__max_depth': list(range(3, 12)),\n",
    "     'reg__n_estimators': [100, 200, 300, 400, 500, 1000]},\n",
    "    X_train,\n",
    "    numpy.log1p(y2_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y2_dev + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T16:19:11.136258Z",
     "start_time": "2017-08-19T15:52:57.810843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casual user RandomForestRegressor\n",
      "Best Parameters: {'reg__min_samples_split': 11, 'reg__n_estimators': 2000}\n",
      "RMSLE: 0.13989044089880226\n",
      "Registered user RandomForestRegressor\n",
      "Best Parameters: {'reg__min_samples_split': 8, 'reg__n_estimators': 500}\n",
      "RMSLE: 0.0769744595514548\n"
     ]
    }
   ],
   "source": [
    "print('Casual user RandomForestRegressor')\n",
    "casual_rf_hyperparameters = simple_grid_search(\n",
    "    RandomForestRegressor(), \n",
    "    simple_feature_eng,\n",
    "    {'reg__min_samples_split': list(range(3, 12)),\n",
    "     'reg__n_estimators': [100, 500, 1000, 2000, 5000]},\n",
    "    X_train,\n",
    "    numpy.log1p(y1_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y1_dev + 1))\n",
    "\n",
    "print('Registered user RandomForestRegressor')\n",
    "registered_rf_hyperparameters = simple_grid_search(\n",
    "    RandomForestRegressor(), \n",
    "    simple_feature_eng,\n",
    "    {'reg__min_samples_split': list(range(3, 12)),\n",
    "     'reg__n_estimators': [100, 500, 1000, 2000, 5000]},\n",
    "    X_train,\n",
    "    numpy.log1p(y2_train + 1),\n",
    "    X_dev,\n",
    "    numpy.log1p(y2_dev + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T16:19:11.153217Z",
     "start_time": "2017-08-19T16:19:11.148456Z"
    }
   },
   "source": [
    "#####  Third Submission Generation\n",
    "Generate our third submission to Kaggle for a score. This requires training 4 models: 2 for the casual users and 2 for the registered users. These models each use the optimal hyperparameters found above. The results of each casual and registered model are averaged together, and then summed to get a final score for 'count'. This resulted in a submission score of ***.44112***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T16:22:40.329091Z",
     "start_time": "2017-08-19T16:21:31.237564Z"
    }
   },
   "outputs": [],
   "source": [
    "# best hyperparameters found from simple_grid_search above\n",
    "casual_gb_max_depth = 6\n",
    "casual_gb_n_estimators = 100\n",
    "registered_gb_max_depth = 3\n",
    "registered_gb_n_estimators = 300\n",
    "\n",
    "casual_rf_min_samples_split = 11\n",
    "casual_rf_n_estimators = 2000\n",
    "registered_rf_min_samples_split = 8\n",
    "registered_rf_n_estimators = 500\n",
    "\n",
    "casual_gb_regressor = GradientBoostingRegressor(n_estimators=casual_gb_n_estimators, \n",
    "                                                max_depth=casual_gb_max_depth)\n",
    "registered_gb_regressor = GradientBoostingRegressor(n_estimators=registered_gb_n_estimators, \n",
    "                                                    max_depth=registered_gb_max_depth)\n",
    "casual_rf_regressor = RandomForestRegressor(min_samples_split=casual_rf_min_samples_split,\n",
    "                                             n_estimators=casual_rf_n_estimators)\n",
    "registered_rf_regressor = RandomForestRegressor(min_samples_split=registered_rf_min_samples_split,\n",
    "                                                 n_estimators=registered_rf_n_estimators)\n",
    "\n",
    "# train the final models on the transformed data and the transformed labels\n",
    "casual_gb_regressor.fit(simple_feature_eng(X), \n",
    "                        numpy.log1p(y1 + 1))\n",
    "casual_rf_regressor.fit(simple_feature_eng(X), \n",
    "                        numpy.log1p(y1 + 1))\n",
    "registered_gb_regressor.fit(simple_feature_eng(X), \n",
    "                            numpy.log1p(y2 + 1))\n",
    "registered_rf_regressor.fit(simple_feature_eng(X), \n",
    "                            numpy.log1p(y2 + 1))\n",
    "\n",
    "# create a dataframe containing the datetimes to predict\n",
    "predictions = pandas.DataFrame(test_data['datetime'])\n",
    "\n",
    "# predict for both the casual and registered users and reverse the transformation\n",
    "casual_gb_predictions = numpy.exp(casual_gb_regressor.predict(simple_feature_eng(test_data))) - 1\n",
    "casual_rf_predictions = numpy.exp(casual_rf_regressor.predict(simple_feature_eng(test_data))) - 1\n",
    "registered_gb_predictions = numpy.exp(registered_gb_regressor.predict(simple_feature_eng(test_data))) - 1\n",
    "registered_rf_predictions = numpy.exp(registered_rf_regressor.predict(simple_feature_eng(test_data))) - 1\n",
    "\n",
    "# the casual and registered predictions are half of the sum from the two models\n",
    "casual_predictions = (casual_gb_predictions + casual_rf_predictions) / 2.0\n",
    "registered_predictions = (registered_gb_predictions + registered_rf_predictions) / 2.0\n",
    "\n",
    "# add the casual and registered predictions and convert to an integer\n",
    "predictions['count'] = (casual_predictions + registered_predictions).astype('int')\n",
    "\n",
    "# create a submission file from the result tagged with the current time\n",
    "predictions.to_csv('submissions/submission{0}.csv'.format(str(int(time.time()))), \n",
    "                   sep=',', \n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Improved Feature Engineering\n",
    "Possible Additions\n",
    "* Normalize parameters\n",
    "* Remove outliers\n",
    "* Add categorical parameters\n",
    "* Create bins for some parameters (hour, windspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T12:26:54.116386Z",
     "start_time": "2017-08-19T12:26:36.056466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
